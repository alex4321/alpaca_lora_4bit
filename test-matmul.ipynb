{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fastchat.conversation import Conversation, SeparatorStyle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton not found. Please run \"pip install triton\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alex4321\\anaconda3\\envs\\llama\\lib\\site-packages\\tqdm-4.65.0-py3.10.egg\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA implementation.\n"
     ]
    }
   ],
   "source": [
    "from alpaca_lora_4bit.autograd_4bit import load_llama_model_4bit_low_ram, Autograd4bitQuantLinear, switch_backend_to\n",
    "switch_backend_to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alpaca_lora_4bit.matmul_utils_4bit\n",
    "alpaca_lora_4bit.matmul_utils_4bit.act_order = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "The safetensors archive passed at vicuna-7B-GPTQ-4bit-128g/vicuna-7B-GPTQ-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model in 4.65 seconds.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_llama_model_4bit_low_ram(\"vicuna-7B-GPTQ-4bit-128g\",\n",
    "                                                 \"vicuna-7B-GPTQ-4bit-128g/vicuna-7B-GPTQ-4bit-128g.safetensors\",\n",
    "                                                 groupsize=128)\n",
    "model.half()\n",
    "for n, m in model.named_modules():\n",
    "    if isinstance(m, Autograd4bitQuantLinear):\n",
    "        if m.is_v1_model:\n",
    "            m.zeros = m.zeros.half()\n",
    "        m.scales = m.scales.half()\n",
    "        m.bias = m.bias.half()\n",
    "from alpaca_lora_4bit.amp_wrapper import AMPWrapper\n",
    "wrapper = AMPWrapper(model)\n",
    "wrapper.apply_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISABLED-FASTER 1.0654296875 \n",
      "DISABLED-OLD FASTER 0.86083984375 \n",
      "FASTER-OLD FASTER 0.90478515625\n",
      "DISABLED OUTPUT (5% - 95% quantiles) -2.06591796875 2.02783203125\n",
      "\n",
      "\n",
      "\n",
      "DISABLED-FASTER 1.0927734375 \n",
      "DISABLED-OLD FASTER 0.93994140625 \n",
      "FASTER-OLD FASTER 0.86962890625\n",
      "DISABLED OUTPUT (5% - 95% quantiles) -2.06787109375 2.0029296875\n",
      "\n",
      "\n",
      "\n",
      "DISABLED-FASTER 1.20703125 \n",
      "DISABLED-OLD FASTER 0.9873046875 \n",
      "FASTER-OLD FASTER 0.99951171875\n",
      "DISABLED OUTPUT (5% - 95% quantiles) -1.97216796875 2.03076171875\n",
      "\n",
      "\n",
      "\n",
      "DISABLED-FASTER 1.0576171875 \n",
      "DISABLED-OLD FASTER 0.85595703125 \n",
      "FASTER-OLD FASTER 0.86328125\n",
      "DISABLED OUTPUT (5% - 95% quantiles) -1.88232421875 1.8505859375\n",
      "\n",
      "\n",
      "\n",
      "DISABLED-FASTER 1.115234375 \n",
      "DISABLED-OLD FASTER 0.98388671875 \n",
      "FASTER-OLD FASTER 0.97265625\n",
      "DISABLED OUTPUT (5% - 95% quantiles) -1.98876953125 1.958251953125\n",
      "\n",
      "\n",
      "\n",
      "DISABLED-FASTER 1.1455078125 \n",
      "DISABLED-OLD FASTER 0.87109375 \n",
      "FASTER-OLD FASTER 0.92919921875\n",
      "DISABLED OUTPUT (5% - 95% quantiles) -2.00439453125 2.01318359375\n",
      "\n",
      "\n",
      "\n",
      "DISABLED-FASTER 1.19140625 \n",
      "DISABLED-OLD FASTER 0.98779296875 \n",
      "FASTER-OLD FASTER 0.90869140625\n",
      "DISABLED OUTPUT (5% - 95% quantiles) -1.967041015625 2.01416015625\n",
      "\n",
      "\n",
      "\n",
      "DISABLED-FASTER 1.025390625 \n",
      "DISABLED-OLD FASTER 0.90966796875 \n",
      "FASTER-OLD FASTER 0.880859375\n",
      "DISABLED OUTPUT (5% - 95% quantiles) -2.080078125 2.04296875\n",
      "\n",
      "\n",
      "\n",
      "DISABLED-FASTER 1.0478515625 \n",
      "DISABLED-OLD FASTER 0.9462890625 \n",
      "FASTER-OLD FASTER 0.90869140625\n",
      "DISABLED OUTPUT (5% - 95% quantiles) -2.04931640625 2.099609375\n",
      "\n",
      "\n",
      "\n",
      "DISABLED-FASTER 1.0419921875 \n",
      "DISABLED-OLD FASTER 0.94677734375 \n",
      "FASTER-OLD FASTER 0.87158203125\n",
      "DISABLED OUTPUT (5% - 95% quantiles) -1.9267578125 1.913330078125\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    torch.manual_seed(42 + i)\n",
    "    x = torch.rand((1, model.config.hidden_size)).half().to(model.device)\n",
    "    alpaca_lora_4bit.matmul_utils_4bit.faster_mode = \"disabled\"\n",
    "    with torch.no_grad():\n",
    "        output_disabled = model.model.layers[0].self_attn.q_proj.forward(x)\n",
    "    alpaca_lora_4bit.matmul_utils_4bit.faster_mode = \"old_faster\"\n",
    "    with torch.no_grad():\n",
    "        output_old_faster = model.model.layers[0].self_attn.q_proj.forward(x)\n",
    "    alpaca_lora_4bit.matmul_utils_4bit.faster_mode = \"faster\"\n",
    "    with torch.no_grad():\n",
    "        output_faster = model.model.layers[0].self_attn.q_proj.forward(x)\n",
    "    mae_to_faster = torch.abs(output_disabled - output_faster).mean()\n",
    "    mae_to_old_faster = torch.abs(output_disabled - output_old_faster).mean()\n",
    "    mae_faster_old_faster = torch.abs(output_old_faster - output_faster).mean()\n",
    "    print(\"DISABLED-FASTER\", mae_to_faster.item(), \"\\nDISABLED-OLD FASTER\", mae_to_old_faster.item(), \"\\nFASTER-OLD FASTER\", mae_faster_old_faster.item())\n",
    "    output_disabled_series = pd.Series(output_disabled.view(-1).detach().cpu().numpy())\n",
    "    print(\"DISABLED OUTPUT (5% - 95% quantiles)\", output_disabled_series.quantile(0.05), output_disabled_series.quantile(0.95))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
