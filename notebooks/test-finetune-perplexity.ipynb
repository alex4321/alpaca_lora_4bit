{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made this notebook to investigate issues with high model perplexity while reducting loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use this model: https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alex4321\\anaconda3\\envs\\llama\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton not found. Please run \"pip install triton\".\n"
     ]
    }
   ],
   "source": [
    "from alpaca_lora_4bit.monkeypatch.peft_tuners_lora_monkey_patch import replace_peft_model_with_int4_lora_model\n",
    "replace_peft_model_with_int4_lora_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA implementation.\n"
     ]
    }
   ],
   "source": [
    "from alpaca_lora_4bit import autograd_4bit\n",
    "autograd_4bit.switch_backend_to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca_lora_4bit.autograd_4bit import load_llama_model_4bit_low_ram\n",
    "from peft import LoraConfig, get_peft_model, get_peft_model_state_dict, PeftModel, set_peft_model_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The safetensors archive passed at ../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model in 6.77 seconds.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_llama_model_4bit_low_ram(\"../vicuna-13b-GPTQ-4bit-128g\",\n",
    "                                                 \"../vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\",\n",
    "                                                 groupsize=128,\n",
    "                                                 is_v1_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS = [\"\"\"\n",
    "The vicuña (Lama vicugna) or vicuna[3] (both /vɪˈkuːnjə/, very rarely spelled vicugna, its former genus name) is one of the two wild South American camelids, which live in the high alpine areas of the Andes, the other being the guanaco, which lives at lower elevations.\n",
    "Vicuñas are relatives of the llama, and are now believed to be the wild ancestor of domesticated alpacas, which are raised for their coats.\n",
    "Vicuñas produce small amounts of extremely fine wool, which is very expensive because the animal can only be shorn every three years and has to be caught from the wild.\n",
    "When knitted together, the product of the vicuña's wool is very soft and warm.\n",
    "The Inca valued vicuñas highly for their wool, and it was against the law for anyone but royalty to wear vicuña garments;\n",
    "today, the vicuña is the national animal of Peru and appears on the Peruvian coat of arms.\n",
    "\n",
    "Both under the rule of the Inca and today, vicuñas have been protected by law, but they were heavily hunted in the intervening period.\n",
    "At the time they were declared endangered in 1974, only about 6,000 animals were left.\n",
    "Today, the vicuña population has recovered to about 350,000, and although conservation organizations have reduced its level of threat classification, they still call for active conservation programs to protect populations from poaching, habitat loss, and other threats.\n",
    "\n",
    "Previously the vicuña was thought not to have been domesticated, and the llama and the alpaca were both regarded as descendants of the closely related guanaco.\n",
    "But DNA research published in 2001 has shown the alpaca may well have vicuña parentage.\n",
    "Today, the vicuña is mainly wild, but the local people still perform special rituals with these creatures, including a fertility rite.\n",
    "\"\"\".strip(),\n",
    "\"\"\"\n",
    "The llama (/ˈlɑːmə/; Spanish pronunciation: [ˈʎama]) (Lama glama) is a domesticated South American camelid, widely used as a meat and pack animal by Andean cultures since the Pre-Columbian era.\n",
    "\n",
    "Llamas are social animals and live with others as a herd.\n",
    "Their wool is soft and contains only a small amount of lanolin.\n",
    "Llamas can learn simple tasks after a few repetitions.\n",
    "When using a pack, they can carry about 25 to 30% of their body weight for 8 to 13 km (5–8 miles).\n",
    "The name llama (in the past also spelled \"lama\" or \"glama\") was adopted by European settlers from native Peruvians.\n",
    "\n",
    "The ancestors of llamas are thought to have originated from the Great Plains of North America about 40 million years ago, and subsequently migrated to South America about three million years ago during the Great American Interchange.\n",
    "By the end of the last ice age (10,000–12,000 years ago), camelids were extinct in North America.\n",
    "As of 2007, there were over seven million llamas and alpacas in South America and over 158,000 llamas and 100,000 alpacas, descended from progenitors imported late in the 20th century, in the United States and Canada.[5]\n",
    "\n",
    "In Aymara mythology, llamas are important beings.\n",
    "The Heavenly Llama is said to drink water from the ocean and urinates as it rains.\n",
    "According to Aymara eschatology, llamas will return to the water springs and lagoons where they come from at the end of time.\n",
    "\"\"\".strip(),\n",
    "\"\"\"\n",
    "The alpaca (Lama pacos) is a species of South American camelid mammal.\n",
    "It is similar to, and often confused with, the llama. However, alpacas are often noticeably smaller than llamas.\n",
    "The two animals are closely related and can successfully crossbreed.\n",
    "Both species are believed to have been domesticated from their wild relatives, the vicuña and guanaco.\n",
    "There are two breeds of alpaca: the Suri alpaca and the Huacaya alpaca.\n",
    "\n",
    "Alpacas are kept in herds that graze on the level heights of the Andes of Southern Peru, Western Bolivia, Ecuador, and Northern Chile at an altitude of 3,500 to 5,000 metres (11,000 to 16,000 feet) above sea level.\n",
    "Alpacas are considerably smaller than llamas, and unlike llamas, they were not bred to be working animals, but were bred specifically for their fiber.\n",
    "Alpaca fiber is used for making knitted and woven items, similar to sheep's wool.\n",
    "These items include blankets, sweaters, hats, gloves, scarves, a wide variety of textiles, and ponchos, in South America, as well as sweaters, socks, coats, and bedding in other parts of the world.\n",
    "The fiber comes in more than 52 natural colors as classified in Peru, 12 as classified in Australia, and 16 as classified in the United States.\n",
    "\n",
    "Alpacas communicate through body language.\n",
    "The most common is spitting to show dominance when they are in distress, fearful, or feel agitated.\n",
    "Male alpacas are more aggressive than females, and tend to establish dominance within their herd group.\n",
    "In some cases, alpha males will immobilize the head and neck of a weaker or challenging male in order to show their strength and dominance.\n",
    "\n",
    "In the textile industry, \"alpaca\" primarily refers to the hair of Peruvian alpacas, but more broadly it refers to a style of fabric originally made from alpaca hair, such as mohair, Icelandic sheep wool, or even high-quality wool from other breeds of sheep.\n",
    "In trade, distinctions are made between alpacas and the several styles of mohair and luster.\n",
    "\n",
    "An adult alpaca generally is between 81 and 99 centimetres (32 and 39 inches) in height at the shoulders (withers).\n",
    "They usually weigh between 48 and 90 kilograms (106 and 198 pounds).\n",
    "Raised in the same conditions, the difference in weight can be small with males weighting around 22.3 kilograms (49 lb 3 oz) and females 21.3 kilograms (46 lb 15 oz).\n",
    "\"\"\".strip()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ExperimentDataset:\n",
    "    def __init__(self, encoded_input_ids) -> None:\n",
    "        self.input_ids = [\n",
    "            item[:-1]\n",
    "            for item in encoded_input_ids\n",
    "        ]\n",
    "        self.labels = [\n",
    "            item[1:]\n",
    "            for item in encoded_input_ids\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"input_ids\": torch.LongTensor(self.input_ids[index]),\n",
    "            \"labels\": torch.LongTensor(self.labels[index]),\n",
    "        }\n",
    "\n",
    "dataset = ExperimentDataset(encoded_input_ids=[\n",
    "    tokenizer(text)['input_ids']\n",
    "    for text in TEXTS\n",
    "])\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_perplexity(model, input_ids, labels):\n",
    "    input_ids_torch = torch.LongTensor([input_ids.tolist()]).to(model.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_sentence = model(input_ids_torch).logits[0]\n",
    "        log_softmax_sentence = torch.nn.functional.log_softmax(logits_sentence, dim=-1)\n",
    "    log_softmax_sentence_np = log_softmax_sentence.detach().cpu().numpy()\n",
    "    log_softmax_correct_tokens = np.array([\n",
    "        log_softmax_sentence_np[i, token]\n",
    "        for i, token in enumerate(labels.tolist())\n",
    "    ])\n",
    "    return np.exp(-log_softmax_correct_tokens.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.494434, 5.0919514, 4.490959]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexities_original = [\n",
    "    model_perplexity(model,\n",
    "                     **dataset[i])\n",
    "    for i in range(len(dataset))\n",
    "]\n",
    "perplexities_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "lora_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in lora_model.modules():\n",
    "    if 'Autograd4bitQuantLinear' in str(type(module)) or 'Linear4bitLt' in str(type(module)):\n",
    "        if hasattr(module, \"is_v1_model\") and module.is_v1_model:\n",
    "            module.zeros = module.zeros.half()\n",
    "        module.scales = module.scales.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Patch Applied For Block 0\n",
      "Forward Patch Applied For Block 1\n",
      "Forward Patch Applied For Block 2\n",
      "Forward Patch Applied For Block 3\n",
      "Forward Patch Applied For Block 4\n",
      "Forward Patch Applied For Block 5\n",
      "Forward Patch Applied For Block 6\n",
      "Forward Patch Applied For Block 7\n",
      "Forward Patch Applied For Block 8\n",
      "Forward Patch Applied For Block 9\n",
      "Forward Patch Applied For Block 10\n",
      "Forward Patch Applied For Block 11\n",
      "Forward Patch Applied For Block 12\n",
      "Forward Patch Applied For Block 13\n",
      "Forward Patch Applied For Block 14\n",
      "Forward Patch Applied For Block 15\n",
      "Forward Patch Applied For Block 16\n",
      "Forward Patch Applied For Block 17\n",
      "Forward Patch Applied For Block 18\n",
      "Forward Patch Applied For Block 19\n",
      "Forward Patch Applied For Block 20\n",
      "Forward Patch Applied For Block 21\n",
      "Forward Patch Applied For Block 22\n",
      "Forward Patch Applied For Block 23\n",
      "Forward Patch Applied For Block 24\n",
      "Forward Patch Applied For Block 25\n",
      "Forward Patch Applied For Block 26\n",
      "Forward Patch Applied For Block 27\n",
      "Forward Patch Applied For Block 28\n",
      "Forward Patch Applied For Block 29\n",
      "Forward Patch Applied For Block 30\n",
      "Forward Patch Applied For Block 31\n",
      "Forward Patch Applied For Block 32\n",
      "Forward Patch Applied For Block 33\n",
      "Forward Patch Applied For Block 34\n",
      "Forward Patch Applied For Block 35\n",
      "Forward Patch Applied For Block 36\n",
      "Forward Patch Applied For Block 37\n",
      "Forward Patch Applied For Block 38\n",
      "Forward Patch Applied For Block 39\n",
      "Var Wrapper Patch Applied\n"
     ]
    }
   ],
   "source": [
    "from alpaca_lora_4bit.gradient_checkpointing import apply_gradient_checkpointing\n",
    "apply_gradient_checkpointing(lora_model, checkpoint_ratio=1.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.494434, 5.0919514, 4.490959]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexities_nontuned_lora = [\n",
    "    model_perplexity(lora_model,\n",
    "                     **dataset[i])\n",
    "    for i in range(len(dataset))\n",
    "]\n",
    "perplexities_nontuned_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=10,\n",
    "    optim=\"adamw_torch\",\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    output_dir=\"test-finetune-loss-perplexity\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    args=training_arguments,\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.01s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 13.802539825439453,\n",
       " 'eval_runtime': 3.1989,\n",
       " 'eval_samples_per_second': 0.938,\n",
       " 'eval_steps_per_second': 0.938}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 14.1635, 'learning_rate': 2e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13.9738, 'learning_rate': 4e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13.2647, 'learning_rate': 6e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:44<06:36, 14.68s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|██████▋   | 2/3 [00:02<00:01,  1.11s/it]\n",
      "                                              \n",
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 13.769719123840332, 'eval_runtime': 10.849, 'eval_samples_per_second': 0.277, 'eval_steps_per_second': 0.277, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:54<06:36, 14.68s/it]\n",
      "100%|██████████| 3/3 [00:10<00:00,  1.50s/it]\n",
      "                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13.2408, 'learning_rate': 8e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 14.0819, 'learning_rate': 0.0001, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13.8274, 'learning_rate': 0.00012, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [01:41<07:28, 18.67s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|██████▋   | 2/3 [00:02<00:01,  1.11s/it]\n",
      "                                              \n",
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 13.517670631408691, 'eval_runtime': 10.625, 'eval_samples_per_second': 0.282, 'eval_steps_per_second': 0.282, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [01:52<07:28, 18.67s/it]\n",
      "100%|██████████| 3/3 [00:10<00:00,  1.50s/it]\n",
      "                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13.7111, 'learning_rate': 0.00014, 'epoch': 2.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.812, 'learning_rate': 0.00016, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13.4376, 'learning_rate': 0.00018, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [02:36<05:42, 16.32s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|██████▋   | 2/3 [00:01<00:00,  1.00it/s]\n",
      "                                              \n",
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 12.755642890930176, 'eval_runtime': 9.797, 'eval_samples_per_second': 0.306, 'eval_steps_per_second': 0.306, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [02:46<05:42, 16.32s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  1.37s/it]\n",
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.2445, 'learning_rate': 0.0002, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12.5007, 'learning_rate': 0.00019, 'epoch': 3.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.9984, 'learning_rate': 0.00018, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [03:30<04:55, 16.39s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|██████▋   | 2/3 [00:02<00:00,  1.00it/s]\n",
      "                                               \n",
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 11.123531341552734, 'eval_runtime': 10.242, 'eval_samples_per_second': 0.293, 'eval_steps_per_second': 0.293, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [03:40<04:55, 16.39s/it]\n",
      "100%|██████████| 3/3 [00:10<00:00,  1.36s/it]\n",
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.7193, 'learning_rate': 0.00017, 'epoch': 4.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.787, 'learning_rate': 0.00016, 'epoch': 4.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.0444, 'learning_rate': 0.00015000000000000001, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [04:26<04:15, 17.04s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|██████▋   | 2/3 [00:02<00:01,  1.11s/it]\n",
      "                                               \n",
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.402701377868652, 'eval_runtime': 10.879, 'eval_samples_per_second': 0.276, 'eval_steps_per_second': 0.276, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [04:37<04:15, 17.04s/it]\n",
      "100%|██████████| 3/3 [00:10<00:00,  1.51s/it]\n",
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.6541, 'learning_rate': 0.00014, 'epoch': 5.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.851, 'learning_rate': 0.00013000000000000002, 'epoch': 5.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.3744, 'learning_rate': 0.00012, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [05:21<03:11, 15.99s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|██████▋   | 2/3 [00:02<00:01,  1.03s/it]\n",
      "                                               \n",
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.072030067443848, 'eval_runtime': 10.3471, 'eval_samples_per_second': 0.29, 'eval_steps_per_second': 0.29, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [05:32<03:11, 15.99s/it]\n",
      "100%|██████████| 3/3 [00:10<00:00,  1.44s/it]\n",
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.9311, 'learning_rate': 0.00012, 'epoch': 6.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.0047, 'learning_rate': 0.00011000000000000002, 'epoch': 6.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.9054, 'learning_rate': 0.0001, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [06:16<02:42, 18.03s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|██████▋   | 2/3 [00:02<00:01,  1.10s/it]\n",
      "                                               \n",
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.380306243896484, 'eval_runtime': 10.87, 'eval_samples_per_second': 0.276, 'eval_steps_per_second': 0.276, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [06:27<02:42, 18.03s/it]\n",
      "100%|██████████| 3/3 [00:10<00:00,  1.51s/it]\n",
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.535, 'learning_rate': 9e-05, 'epoch': 7.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.998, 'learning_rate': 8e-05, 'epoch': 7.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8275, 'learning_rate': 7e-05, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [07:13<01:40, 16.74s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|██████▋   | 2/3 [00:02<00:01,  1.02s/it]\n",
      "                                               \n",
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.599062442779541, 'eval_runtime': 9.866, 'eval_samples_per_second': 0.304, 'eval_steps_per_second': 0.304, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [07:23<01:40, 16.74s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  1.38s/it]\n",
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5698, 'learning_rate': 6e-05, 'epoch': 8.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.4273, 'learning_rate': 5e-05, 'epoch': 8.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3603, 'learning_rate': 4e-05, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [08:05<00:53, 17.69s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|██████▋   | 2/3 [00:02<00:01,  1.00s/it]\n",
      "                                               \n",
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.262597560882568, 'eval_runtime': 9.755, 'eval_samples_per_second': 0.308, 'eval_steps_per_second': 0.308, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [08:15<00:53, 17.69s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  1.36s/it]\n",
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3089, 'learning_rate': 3e-05, 'epoch': 9.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.2346, 'learning_rate': 2e-05, 'epoch': 9.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1522, 'learning_rate': 1e-05, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [08:58<00:00, 15.91s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 67%|██████▋   | 2/3 [00:02<00:01,  1.12s/it]\n",
      "                                               \n",
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.188497066497803, 'eval_runtime': 10.8965, 'eval_samples_per_second': 0.275, 'eval_steps_per_second': 0.275, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [09:09<00:00, 15.91s/it]\n",
      "100%|██████████| 3/3 [00:10<00:00,  1.51s/it]\n",
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 549.4434, 'train_samples_per_second': 0.055, 'train_steps_per_second': 0.055, 'train_loss': 10.031383641560872, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [09:09<00:00, 18.31s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=10.031383641560872, metrics={'train_runtime': 549.4434, 'train_samples_per_second': 0.055, 'train_steps_per_second': 0.055, 'train_loss': 10.031383641560872, 'epoch': 10.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:10<00:00,  3.62s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 6.188497066497803,\n",
       " 'eval_runtime': 10.955,\n",
       " 'eval_samples_per_second': 0.274,\n",
       " 'eval_steps_per_second': 0.274,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26.571615, 23.290499, 33.504417]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexities_nontuned_lora = [\n",
    "    model_perplexity(lora_model,\n",
    "                     **dataset[i])\n",
    "    for i in range(len(dataset))\n",
    "]\n",
    "perplexities_nontuned_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So:\n",
    "- initial model perplexities on this texts were: \n",
    "    `[6.494379, 5.0919175, 4.490609]`\n",
    "- perplexities after applying LoRA adapters, but before finetuning (basically the same model with some noise?)\n",
    "    `[6.494379, 5.0919175, 4.490609]`\n",
    "- loss of LoRA-applied model before finetuning:\n",
    "    ```\n",
    "    {'eval_loss': 13.802539825439453,\n",
    "    'eval_runtime': 2.9367,\n",
    "    'eval_samples_per_second': 1.022,\n",
    "    'eval_steps_per_second': 1.022}\n",
    "    ```\n",
    "- loss of LoRA model after trying to overfit for this texts:\n",
    "    ```\n",
    "    {'eval_loss': 6.17491340637207,\n",
    "    'eval_runtime': 26.0902,\n",
    "    'eval_samples_per_second': 0.115,\n",
    "    'eval_steps_per_second': 0.115,\n",
    "    'epoch': 10.0}\n",
    "    ```\n",
    "- perplexities\n",
    "    ```\n",
    "    [26.089653, 23.530462, 32.777637]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: peft\n",
      "Version: 0.3.0\n",
      "Summary: Parameter-Efficient Fine-Tuning (PEFT)\n",
      "Home-page: https://github.com/huggingface/peft\n",
      "Author: The HuggingFace team\n",
      "Author-email: sourab@huggingface.co\n",
      "License: Apache\n",
      "Location: C:\\Users\\alex4321\\anaconda3\\envs\\llama\\Lib\\site-packages\n",
      "Requires: accelerate, numpy, packaging, psutil, pyyaml, torch, transformers\n",
      "Required-by: alpaca-lora-4bit\n",
      "Name: transformers\n",
      "Version: 4.28.1\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: C:\\Users\\alex4321\\anaconda3\\envs\\llama\\Lib\\site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, tokenizers, tqdm\n",
      "Required-by: alpaca-lora-4bit, peft\n",
      "Name: alpaca-lora-4bit\n",
      "Version: 0.1.2\n",
      "Summary: Alpaca LoRA 4-bit\n",
      "Home-page: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Author: \n",
      "Author-email: \n",
      "License: \n",
      "Location: C:\\Users\\alex4321\\anaconda3\\envs\\llama\\Lib\\site-packages\n",
      "Requires: accelerate, colorama, datasets, einops, packaging, peft, pyzmq, safetensors, sentencepiece, torch, transformers, wandb\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show peft\n",
    "!pip show transformers\n",
    "!pip show alpaca-lora-4bit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
